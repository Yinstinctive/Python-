##### Introduction
Web scraping involves programmatically grabbing data from a web page<br>
Three steps: Download, extract data, do something with data<br>
##### Why Scrape?
There's data on a site that you want to store or analyze<br>
You can't get by other means (e.g. an API)<br>
You want to prgrammatically grab the data (instead of lots of manual copying/pasting)<br>
##### Is it...OK?
Some websites don't want people scraping them<br>
Best practice: consult the robots.txt file<br>
If making making many requests, time them out<br>
If you're too aggressive, your IP can be blocked<br>
##### Beautiful Soup
python3 -m pip install bs4<br>
To extract data  from HTML, we'll use Beautiful Soup<br>
Install it with pip<br>
Beautiful Soup lets us navigate through HTML with Python<br>
Beautiful Soup does not download HTML - for this, we need the requests module<br>
##### Parsing and Navigating 
HTMLBeautifulSoup(html_string, "html.parser") - parse HTML<br>
Once parsed, there are serveral ways to navigate:
- By Tag Name
- Using find - returns one matching tag
- Using find_all - returns a list of matching tags

##### Navigating with CSS Selectors
select - returns a list of elements matching a CSS<br>
selectorSelector Cheatsheet
- Select by id of foo: #foo
- Select by class of bar: .bar
- Select children: div > p
- Select descendants: div p

##### Accessing Data in Elements
get_text - access the inner text in an element
name - tag name
attrs - dictionary of attributes
You can also access attribute values using brackets
##### Navigating with Beautiful Soup Via Tags
parent / parents
contents
next_sibling / next_siblings
previous_sibling / previous_siblings
##### Via Searching
find_parent / find_parents
find_next_sibling / find_next_siblings
find_previous_sibling / find_previous_siblings
Creating a Web Crawler with Scrapy
python -m pip install scrapyscrapy 
runspider -o books.csv book_scraper.py
```Python
import scrapy

class BookSpider(scrapy.Spider):
    name = 'bookspider'
    start_urls = ['http://books.toscrape.com/']

    def parse(self,response):
        for article in response.css('article.product_pod')
            yield {
                'price': article.css(".price_color:text").extract_first(),
                'title': article.css("h3 > a::attr(title)").extract_first()
            }
            next = response.css('.next > a ::attr(href)').extract_first()
            if next:
                yield reponse.follow(next, self.parse)
```